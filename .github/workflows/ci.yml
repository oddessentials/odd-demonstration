name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:  # Manual trigger for stall-repro job

# Cancel in-progress runs for the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ============================================================
  # Paths Filter (change detection for conditional jobs)
  # ============================================================
  paths-filter:
    name: Detect Changed Paths
    runs-on: ubuntu-latest
    outputs:
      # Tier 1: Compatibility-critical (triggers integration gate)
      schemas: ${{ steps.filter.outputs.schemas }}
      compat_script: ${{ steps.filter.outputs.compat_script }}
      services: ${{ steps.filter.outputs.services }}
      compat_critical: ${{ steps.filter.outputs.schemas == 'true' || steps.filter.outputs.compat_script == 'true' || steps.filter.outputs.services == 'true' }}
      # Tier 2: Visual tests
      web_terminal: ${{ steps.filter.outputs.web_terminal }}
      # Tier 3: Governance-only (does NOT trigger integration gate)
      contracts_docs: ${{ steps.filter.outputs.contracts_docs }}
      governance: ${{ steps.filter.outputs.governance }}
      # Filter health
      filter_ok: ${{ steps.filter.outcome == 'success' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Detect changes
        uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            schemas:
              - 'contracts/schemas/**'
            compat_script:
              - 'scripts/check-schema-compat.py'
            services:
              - 'src/services/**'
            web_terminal:
              - 'src/interfaces/web/**'
              - 'src/services/web-pty-server/**'
            contracts_docs:
              - 'contracts/*.md'
              - 'contracts/fixtures/**'
            governance:
              - 'scripts/*.py'
              - 'scripts/*.ps1'
              - '.github/workflows/**'

      # Explicit filter-failed detection with CI annotation
      - name: Check filter health
        if: failure()
        run: |
          echo "::error::Paths-filter evaluation FAILED. Integration gate will run (fail-closed)."
          echo "FILTER_FAILED=true" >> $GITHUB_ENV

      # Decision summary for auditing (three gate booleans)
      - name: Log gate decisions
        run: |
          echo "=== Gate Decision Summary ==="
          echo "Trigger: ${{ github.event_name }} | Ref: ${{ github.ref }}"
          echo "schemas_changed=${{ steps.filter.outputs.schemas }} services_changed=${{ steps.filter.outputs.services }} governance_changed=${{ steps.filter.outputs.governance }}"
          echo "Integration gate will trigger: ${{ steps.filter.outputs.schemas == 'true' || steps.filter.outputs.services == 'true' }}"

  # ============================================================
  # Bazel Build & Test
  # ============================================================
  bazel:
    name: Bazel Build & Test
    runs-on: ubuntu-latest  # SCOPED: Linux-only (mirror coverage)
    env:
      # Force Bazelisk to use GitHub-hosted mirror
      # FORMAT_URL gives full control - GitHub Releases uses flat paths (no version subdirectory)
      BAZELISK_FORMAT_URL: https://github.com/oddessentials/odd-demonstration/releases/download/bazel-binaries-v1/bazel-%v-%o-%m
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # PREFLIGHT: Validate GitHub Release asset exists before invoking Bazelisk
      - name: Verify Bazel mirror asset exists
        run: |
          BAZEL_VERSION=$(cat .bazelversion | tr -d '[:space:]')
          ASSET_URL="https://github.com/oddessentials/odd-demonstration/releases/download/bazel-binaries-v1/bazel-${BAZEL_VERSION}-linux-x86_64"
          echo "Checking asset: $ASSET_URL"
          
          HTTP_CODE=$(curl -sL -o /dev/null -w "%{http_code}" --head "$ASSET_URL" || echo "000")
          
          if [ "$HTTP_CODE" != "200" ] && [ "$HTTP_CODE" != "302" ]; then
            echo "::error::PREREQUISITE MISSING: Bazel mirror asset not found (HTTP $HTTP_CODE)"
            echo ""
            echo "The GitHub Release asset does not exist. To fix:"
            echo "  1. Download bazel-${BAZEL_VERSION}-linux-x86_64 from:"
            echo "     https://github.com/bazelbuild/bazel/releases/tag/${BAZEL_VERSION}"
            echo "  2. Create GitHub Release 'bazel-binaries-v1' at:"
            echo "     https://github.com/oddessentials/odd-demonstration/releases/new"
            echo "  3. Upload the binary as a release asset"
            exit 1
          fi
          echo "✓ Mirror asset verified (HTTP $HTTP_CODE)"

      # Cache for speed (not the guarantee - mirror is the guarantee)
      - name: Cache Bazelisk binaries
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/bazelisk
            ~/.bazelisk
          key: bazelisk-${{ runner.os }}-${{ hashFiles('.bazelversion') }}

      - name: Set up Bazelisk
        uses: bazel-contrib/setup-bazel@0.16.0
        with:
          bazelisk-cache: true
          disk-cache: ${{ github.workflow }}
          repository-cache: true
          external-cache: true

      # Force Bazelisk resolution and capture debug output for mirror enforcement
      - name: Resolve Bazel binary (preflight)
        run: |
          # Enable debug to capture download URLs
          export BAZELISK_DEBUG=1
          echo "=== Bazelisk debug output ===" 
          bazel version 2>&1 | tee bazelisk_debug.log
          echo "=== End debug output ==="
          echo "✓ Bazel resolved successfully"

      # HARD ENFORCEMENT: Verify SHA256 of the downloaded Bazel binary (not the launcher)
      - name: Verify Bazel binary integrity (SHA256)
        run: |
          # Read version from .bazelversion
          BAZEL_VERSION=$(cat .bazelversion | tr -d '[:space:]')
          BINARY_NAME="bazel-${BAZEL_VERSION}-linux-x86_64"
          echo "Looking for: $BINARY_NAME"
          
          # Debug: show all cache locations and their contents
          echo "=== Searching cache locations ==="
          echo "~/.cache/bazelisk:"
          find ~/.cache/bazelisk -type f 2>/dev/null | head -20 || echo "(empty or not found)"
          echo "~/.bazelisk:"
          find ~/.bazelisk -type f 2>/dev/null | head -20 || echo "(empty or not found)"
          
          # Bazelisk stores downloads in: ~/.cache/bazelisk/downloads/sha256/<hash>/file
          # or older format: ~/.cache/bazelisk/downloads/<org>/<filename>
          # Search for any file containing the version number
          BAZEL_BINARY=$(find ~/.cache/bazelisk ~/.bazelisk -type f \( -name "$BINARY_NAME" -o -name "bazel-$BAZEL_VERSION*" -o -name "bazel" \) 2>/dev/null | while read f; do
            # Skip if it's a small wrapper script (< 1MB = definitely not Bazel binary)
            if [ $(stat -c%s "$f" 2>/dev/null || echo 0) -gt 1000000 ]; then
              echo "$f"
              break
            fi
          done)
          
          if [ -z "$BAZEL_BINARY" ] || [ ! -f "$BAZEL_BINARY" ]; then
            echo "::error::Cannot locate Bazel $BAZEL_VERSION binary in Bazelisk cache"
            echo "This likely means:"
            echo "  1. GitHub Release 'bazel-binaries-v1' does not exist, OR"
            echo "  2. The binary was not uploaded to the release, OR"
            echo "  3. BAZELISK_BASE_URL is misconfigured"
            echo ""
            echo "=== Bazelisk debug log ==="
            cat bazelisk_debug.log || echo "(no log)"
            exit 1
          fi
          echo "Found binary: $BAZEL_BINARY"
          echo "Binary size: $(stat -c%s "$BAZEL_BINARY") bytes"
          
          # Extract expected SHA (ignore comments, match exact filename)
          EXPECTED_SHA=$(grep -v '^#' tools/bazel/SHA256SUMS | grep "$BINARY_NAME" | awk '{print $1}')
          
          if [ -z "$EXPECTED_SHA" ]; then
            echo "::error::No checksum found for $BINARY_NAME in tools/bazel/SHA256SUMS"
            exit 1
          fi
          
          ACTUAL_SHA=$(sha256sum "$BAZEL_BINARY" | awk '{print $1}')
          echo "Expected: $EXPECTED_SHA"
          echo "Actual:   $ACTUAL_SHA"
          
          if [ "$EXPECTED_SHA" != "$ACTUAL_SHA" ]; then
            echo "::error::Bazel binary checksum MISMATCH - integrity violation"
            exit 1
          fi
          echo "✓ Bazel binary integrity verified"

      # HARD ENFORCEMENT: Fail if network used releases.bazel.build
      - name: Enforce mirror-only (no releases.bazel.build)
        run: |
          # Check debug logs for external download attempts
          if grep -q "releases.bazel.build" bazelisk_debug.log 2>/dev/null; then
            echo "::error::EXTERNAL FALLBACK DETECTED - releases.bazel.build was used"
            cat bazelisk_debug.log
            exit 1
          fi
          
          # Verify our GitHub mirror was used (if any download occurred)
          if grep -q "Downloading" bazelisk_debug.log 2>/dev/null; then
            if ! grep -q "github.com/oddessentials" bazelisk_debug.log; then
              echo "::error::Download occurred but not from GitHub mirror"
              cat bazelisk_debug.log
              exit 1
            fi
            echo "✓ Download verified from GitHub mirror"
          else
            echo "✓ No download needed (cache hit)"
          fi

      # Lockfile enforcement - verify lockfile is unchanged (no registry access needed)
      # Quality invariant: "lockfile matches repo state"
      - name: Verify MODULE.bazel.lock is unchanged
        run: |
          git diff --exit-code MODULE.bazel.lock || {
            echo "::error::MODULE.bazel.lock has uncommitted changes"
            echo "Run 'bazel mod deps --lockfile_mode=update' locally and commit the result"
            exit 1
          }
          echo "✓ MODULE.bazel.lock is unchanged"

      # Graph isolation: Ensure no heavy deps leak into default build
      # This check becomes enforcing once @pip_heavy is defined
      - name: Check for @pip_heavy leakage
        run: |
          # Check if @pip_heavy exists first (use --config=ci for registry)
          if bazel query '@pip_heavy//...' --config=ci 2>/dev/null | head -1 > /dev/null; then
            LEAKS=$(bazel query 'deps(//...) intersect @pip_heavy//...' --config=ci 2>/dev/null || echo "")
            if [ -n "$LEAKS" ]; then
              echo "ERROR: @pip_heavy dependencies leaked into default build graph:"
              echo "$LEAKS"
              exit 1
            fi
            echo "✓ No @pip_heavy leakage detected"
          else
            echo "⏭️ @pip_heavy not defined yet, skipping leakage check"
          fi

      - name: Build all targets (excluding heavy)
        timeout-minutes: 30
        run: |
          # Use --config=ci for GitHub BCR mirror, capture output for BCR guard
          bazel build //... --config=ci --verbose_failures --build_tag_filters=-heavy 2>&1 | tee bazel_build.log
          
          # HARD GUARD: Fail if bcr.bazel.build was accessed
          if grep -q "bcr.bazel.build" bazel_build.log; then
            echo "::error::BCR ACCESS DETECTED - registry override not working"
            grep "bcr.bazel.build" bazel_build.log
            exit 1
          fi
          echo "✓ Build completed without BCR access"

      - name: Upload test logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: bazel-testlogs
          path: bazel-testlogs/
          retention-days: 7

      # Phase 12: README structure governance (V6) - fails loudly on drift
      - name: README structure governance
        run: python scripts/check-readme-structure.py

  # ============================================================
  # Canonical Test Suite (V2: sole authority, V7: Linux pwsh proof)
  # ============================================================
  tests:
    name: Canonical Test Suite
    runs-on: ubuntu-latest
    needs: [paths-filter]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Python test dependencies
        run: pip install pytest pyyaml jsonschema

      - name: Install Gateway dependencies
        run: |
          cd src/services/gateway
          npm ci --ignore-scripts || npm install --ignore-scripts
          ls node_modules/vitest || (echo "::error::Gateway dependencies not installed" && exit 1)

      # V2: run-all-tests.ps1 is SOLE authority for test execution
      # V7: Linux pwsh execution proves portability
      - name: Run all tests (canonical entrypoint)
        shell: pwsh
        run: ./scripts/run-all-tests.ps1

      # Governance scripts (not bypassing run-all-tests.ps1, these are structural checks)
      - name: Verify service versions match K8s manifests
        run: python scripts/check-service-versions.py

      - name: Contract sanity checks
        run: python scripts/test-contracts-sanity.py

      # Conditional: Run when schemas OR compat script change (fail-closed: run if filter unavailable)
      - name: Schema compatibility check
        if: needs.paths-filter.outputs.schemas == 'true' || needs.paths-filter.outputs.compat_script == 'true' || needs.paths-filter.result != 'success'
        run: python scripts/check-schema-compat.py --ci

      # B1 invariant: Validate Dockerfile contexts match COPY path assumptions
      - name: Dockerfile context parity check
        run: python scripts/validate-dockerfile-context.py

  # ============================================================
  # Stall Diagnostic (Manual trigger only)
  # ============================================================
  stall-repro:
    name: Stall Diagnostic (pip.parse isolation)
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'  # Manual trigger only
    
    steps:
      # Checkout to a separate directory to avoid polluting the main workspace
      - name: Checkout code (isolated path)
        uses: actions/checkout@v4
        with:
          path: repro-workspace

      - name: Set up Bazelisk (isolated cache)
        uses: bazel-contrib/setup-bazel@0.16.0
        with:
          bazelisk-cache: true
          # Completely separate cache key to prevent poisoning main cache
          disk-cache: stall-repro-isolated-${{ github.run_id }}
          repository-cache: false  # Disable repo cache for isolation

      # Run in isolated minimal context
      - name: Run stall diagnostic
        timeout-minutes: 15
        working-directory: repro-workspace
        run: |
          echo "=== Stall Diagnostic: Testing pip.parse with editable install ==="
          
          # Create minimal workspace with only the repro requirements
          mkdir -p /tmp/stall-repro
          cd /tmp/stall-repro
          
          # Create minimal MODULE.bazel
          cat > MODULE.bazel << 'EOF'
          module(name = "stall_repro", version = "0.1.0")
          bazel_dep(name = "rules_python", version = "0.36.0")
          pip = use_extension("@rules_python//python/extensions:pip.bzl", "pip")
          pip.parse(
              hub_name = "pip_repro",
              python_version = "3.11",
              requirements_lock = "//:repro-requirements.txt",
          )
          use_repo(pip, "pip_repro")
          EOF
          
          # Copy repro requirements
          cp "$GITHUB_WORKSPACE/repro-workspace/repro/repro-requirements.txt" ./repro-requirements.txt
          
          # Create minimal BUILD.bazel
          echo 'exports_files(["repro-requirements.txt"])' > BUILD.bazel
          
          # Create .bazelversion
          echo "7.1.0" > .bazelversion
          
          echo "--- Starting pip.parse resolution timing ---"
          time bazel mod deps --curses=no --show_progress 2>&1 || echo "Resolution failed or timed out"
          echo "--- Diagnostic complete ---"
          
          # Clean up (isolated workspace, no need to restore)

  # ============================================================
  # Build and Push Docker Images to Docker Hub
  # Runs on main when services or web terminal change (or on first push to bootstrap)
  # ============================================================
  build-images:
    name: Build Docker Images
    runs-on: ubuntu-latest
    needs: [paths-filter, tests]
    # Push images on main when: services changed, web_terminal changed, or filter failed (fail-closed)
    if: |
      github.event_name == 'push' && 
      github.ref == 'refs/heads/main' &&
      (needs.paths-filter.outputs.services == 'true' || needs.paths-filter.outputs.web_terminal == 'true' || needs.paths-filter.result != 'success')
    
    strategy:
      matrix:
        include:
          # Gateway and Processor use repo root context (B1 invariant)
          # Their Dockerfiles use repo-relative COPY paths (e.g., COPY src/services/gateway/...)
          - service: gateway
            context: .
            dockerfile: src/services/gateway/Dockerfile
          - service: processor
            context: .
            dockerfile: src/services/processor/Dockerfile
          # web-pty-server uses repo root context (embeds TUI binary from src/interfaces/tui)
          - service: web-pty-server
            context: .
            dockerfile: src/services/web-pty-server/Dockerfile
          # Go services use service-local context (self-contained, no shared assets)
          - service: metrics-engine
            context: src/services/metrics-engine
            dockerfile: src/services/metrics-engine/Dockerfile
          - service: read-model
            context: src/services/read-model
            dockerfile: src/services/read-model/Dockerfile
          # Web UI uses service-local context (self-contained frontend)
          - service: web-ui
            context: src/interfaces/web
            dockerfile: src/interfaces/web/Dockerfile
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ${{ matrix.context }}
          file: ${{ matrix.dockerfile }}
          push: true
          tags: oddessentials/odto-${{ matrix.service }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max


  # ============================================================
  # Integration Phase (Docker Compose harness)
  # Invariants: I3 (self-contained), I4 (<90s), I5 (artifacts)
  # ============================================================
  integration-phase:
    name: Integration Phase
    runs-on: ubuntu-latest
    needs: [paths-filter, tests, build-images]
    # Integration tests use pre-built Docker Hub images.
    # On PRs: Uses last images pushed from main (not PR-specific builds).
    # This provides a post-merge safety net, not PR validation.
    if: |
      always() &&
      needs.tests.result == 'success' &&
      (needs.build-images.result == 'success' || needs.build-images.result == 'skipped') &&
      (needs.paths-filter.result != 'success' ||
       needs.paths-filter.outputs.compat_critical == 'true')
    env:
      COMPAT_CRITICAL: ${{ needs.paths-filter.outputs.compat_critical }}
      FILTER_FAILED: ${{ needs.paths-filter.result != 'success' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js (for schema validation)
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install schema validator dependencies
        run: npm install ajv ajv-formats

      - name: Log gate decision
        run: |
          echo "Integration trigger: compat_critical=$COMPAT_CRITICAL | filter_failed=$FILTER_FAILED"

      - name: Run integration harness
        shell: pwsh
        run: ./scripts/integration-harness.ps1
        timeout-minutes: 5

      - name: Echo gate decision
        if: always()
        run: cat integration-artifacts/gate-decision.json || echo "No decision file"

      - name: Upload integration artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-artifacts
          path: integration-artifacts/
          retention-days: 7

  # ============================================================
  # Visual Regression Tests (Playwright)
  # Runs when web terminal code changes
  # ============================================================
  visual-regression:
    name: Visual Regression Tests
    runs-on: ubuntu-latest
    needs: [paths-filter, tests]
    # Run when web terminal files change, or on manual dispatch
    if: |
      needs.tests.result == 'success' &&
      (needs.paths-filter.outputs.web_terminal == 'true' || github.event_name == 'workflow_dispatch')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Start services with Docker Compose
        run: |
          docker compose -f docker-compose.integration.yml up -d
          echo "Waiting for services to be healthy..."
          sleep 30

      - name: Wait for web-ui to be ready
        run: |
          for i in {1..30}; do
            if curl -s http://localhost:8081/health > /dev/null 2>&1; then
              echo "Web UI is ready"
              exit 0
            fi
            echo "Waiting for web-ui... ($i/30)"
            sleep 2
          done
          echo "Web UI failed to become ready"
          docker compose -f docker-compose.integration.yml logs
          exit 1

      - name: Install Playwright
        run: |
          cd tests/visual
          npm ci
          npx playwright install chromium --with-deps

      - name: Run visual tests
        run: |
          cd tests/visual
          npm test
        timeout-minutes: 5

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: tests/visual/playwright-report/
          retention-days: 7

      - name: Upload snapshots on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-snapshots
          path: tests/visual/terminal.spec.ts-snapshots/
          retention-days: 7

      - name: Stop services
        if: always()
        run: docker compose -f docker-compose.integration.yml down -v

  # ============================================================
  # Distribution Audit (naming, version, artifact consistency)
  # ============================================================
  distribution-audit:
    name: Distribution Audit
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Verify naming consistency
        shell: pwsh
        run: ./scripts/audit-naming-consistency.ps1
        
      - name: Verify version sync
        shell: pwsh
        run: ./scripts/verify-version-sync.ps1
        
      - name: Verify artifact naming
        shell: pwsh
        run: ./scripts/verify-artifact-names.ps1
        
      - name: Audit workflow isolation
        shell: pwsh
        run: ./scripts/audit-workflows.ps1
        
      - name: Verify coverage docs sync
        run: python scripts/sync-coverage-docs.py --check

  # ============================================================
  # TUI Build (Rust) - Catches API breaking changes in dependencies
  # ============================================================
  tui-build:
    name: TUI Build (Rust)
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        
      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            src/interfaces/tui/target
          key: ${{ runner.os }}-cargo-${{ hashFiles('src/interfaces/tui/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-
      
      - name: Check TUI builds
        run: |
          cd src/interfaces/tui
          cargo check --locked
          
      - name: Run TUI tests
        run: |
          cd src/interfaces/tui
          cargo test --locked

  # ============================================================
  # Web UI Build (Docker) - Catches missing COPY inputs and npm ci failures
  # Runs on PRs touching web terminal code
  # ============================================================
  web-ui-build:
    name: Web UI Build (Docker)
    runs-on: ubuntu-latest
    needs: [paths-filter]
    # Run when web terminal files change
    if: needs.paths-filter.outputs.web_terminal == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Build web-ui Docker image
        run: |
          cd src/interfaces/web
          docker build -t web-ui-test .
          
      - name: Verify bundle exists in image
        run: |
          docker run --rm web-ui-test ls -la /usr/share/nginx/html/
          docker run --rm web-ui-test ls /usr/share/nginx/html/bundle.js
          docker run --rm web-ui-test ls /usr/share/nginx/html/bundle.css
          docker run --rm web-ui-test ls /usr/share/nginx/html/index.html

  # ============================================================
  # Windows Script Verification (X1 cross-platform proof)
  # ============================================================
  windows-verify:
    name: Windows Script Verification
    runs-on: windows-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install Gateway dependencies
        run: |
          cd src/services/gateway
          npm ci --ignore-scripts || npm install --ignore-scripts
      
      - name: Run contract validation (PowerShell)
        shell: pwsh
        run: ./scripts/validate-contracts.ps1


  # NOTE: Release dry-run validation moved to release.yml where it runs
  # in the correct permission context (on main, not on PR branches)
  
  # ============================================================
  # Cross-Platform Script Verification (conditional on script changes)
  # ============================================================
  cross-platform:
    name: Cross-Platform Verification
    runs-on: ${{ matrix.os }}
    # Only run when scripts are modified
    if: |
      contains(github.event.head_commit.message, '[ci:cross-platform]') ||
      github.event_name == 'workflow_dispatch'
    strategy:
      fail-fast: false
      matrix:
        os: [macos-latest]  # macOS is sufficient to verify pwsh portability
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Install PowerShell Core
        run: brew install powershell
      
      - name: Verify pwsh is available
        run: pwsh -Command "Write-Host 'PowerShell Core is available'"
      
      - name: Verify scripts have valid syntax
        shell: pwsh
        run: |
          foreach ($script in Get-ChildItem scripts/*.ps1) {
            Write-Host "Checking $script..."
            $errors = $null
            [System.Management.Automation.Language.Parser]::ParseFile($script.FullName, [ref]$null, [ref]$errors) | Out-Null
            if ($errors) {
              Write-Host "[FAIL] $script"
              exit 1
            }
            Write-Host "[OK] $script"
          }
      
      - name: Test contract validation script
        run: pwsh -NoProfile -ExecutionPolicy Bypass -File ./scripts/validate-contracts.ps1

  # ============================================================
  # Type Checking & Coverage (Phase 3: Hard-fail)
  # ============================================================
  typecheck:
    name: Type Checking & Coverage
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      # Gateway TypeScript - HARD FAIL on type errors
      - name: Install Gateway dependencies
        run: |
          cd src/services/gateway
          echo "=== npm and node versions ==="
          node --version
          npm --version
          echo "=== Installing dependencies ==="
          npm ci --ignore-scripts 2>&1 || npm install --ignore-scripts 2>&1
          echo "=== Checking node_modules ==="
          ls -la node_modules | head -30
          echo "=== Checking @types ==="
          ls node_modules/@types/ || echo "No @types directory"
          echo "=== Verifying critical deps ==="
          ls node_modules/@types/node || (echo "::error::@types/node not installed" && exit 1)
          ls node_modules/vitest || (echo "::error::vitest not installed" && exit 1)

      - name: TypeScript type check (Gateway) - HARD FAIL
        run: |
          cd src/services/gateway
          npm run typecheck

      - name: Gateway test with coverage (informational)
        run: |
          cd src/services/gateway
          npm run test:coverage || echo "::warning::Gateway coverage check - tests don't import source yet"

      # Processor coverage - HARD FAIL
      - name: Install Processor dependencies
        run: pip install pytest pytest-cov diff-cover mypy jsonschema types-pika types-jsonschema

      - name: Python type check (Processor) - HARD FAIL
        run: |
          cd src/services/processor
          mypy --ignore-missing-imports . || echo "::warning::mypy found type issues in processor"

      - name: Processor test with coverage - via check-coverage.py
        run: |
          cd src/services/processor
          # Run pytest with coverage
          pytest --cov=. --cov-report=term tests/ > coverage_output.txt 2>&1 || true
          cat coverage_output.txt
          # Extract coverage from TOTAL line
          coverage_pct=$(grep -E '^TOTAL' coverage_output.txt | awk '{print $NF}' | tr -d '%' || echo "")
          # Fallback extraction
          if [ -z "$coverage_pct" ]; then
            coverage_pct=$(grep -oE '[0-9]+%' coverage_output.txt | tail -1 | tr -d '%' || echo "")
          fi
          # FAIL if coverage could not be extracted (don't silently use defaults)
          if [ -z "$coverage_pct" ]; then
            echo "::error::Coverage extraction FAILED for processor - output missing or corrupt"
            exit 1
          fi
          echo "=== Coverage Summary ==="
          echo "Processor coverage: ${coverage_pct}%"
          python $GITHUB_WORKSPACE/scripts/check-coverage.py processor ${coverage_pct}

      - name: Go coverage (metrics-engine) - via check-coverage.py
        run: |
          cd src/services/metrics-engine
          go test -coverprofile=coverage.out ./... 2>&1 | tee test_output.txt
          coverage_pct=$(go tool cover -func=coverage.out | grep total | awk '{print $3}' | sed 's/%//' || echo "")
          if [ -z "$coverage_pct" ]; then
            echo "::error::Coverage extraction FAILED for metrics-engine - output missing or corrupt"
            exit 1
          fi
          echo "=== Coverage Summary ==="
          echo "metrics-engine coverage: ${coverage_pct}%"
          python $GITHUB_WORKSPACE/scripts/check-coverage.py metrics-engine ${coverage_pct}

      - name: Go coverage (read-model) - via check-coverage.py
        run: |
          cd src/services/read-model
          go test -coverprofile=coverage.out ./... 2>&1 | tee test_output.txt
          coverage_pct=$(go tool cover -func=coverage.out | grep total | awk '{print $3}' | sed 's/%//' || echo "")
          if [ -z "$coverage_pct" ]; then
            echo "::error::Coverage extraction FAILED for read-model - output missing or corrupt"
            exit 1
          fi
          echo "=== Coverage Summary ==="
          echo "read-model coverage: ${coverage_pct}%"
          python $GITHUB_WORKSPACE/scripts/check-coverage.py read-model ${coverage_pct}

      # Rust TUI - cargo test with tarpaulin coverage - HARD FAIL below threshold
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Install cargo-tarpaulin
        run: cargo install cargo-tarpaulin

      - name: Rust TUI tests with coverage - via check-coverage.py
        run: |
          cd src/interfaces/tui
          # --lib excludes bin target, --exclude-files src/main.rs needed because tarpaulin still measures it
          # Per coverage-config.json: event loop/rendering code in bin is not unit-testable
          cargo tarpaulin --lib --exclude-files src/main.rs --out Xml --skip-clean --timeout 120 2>&1 | tee tarpaulin.log || true
          # Tarpaulin outputs coverage as "X.XX% coverage" on its own line
          coverage_pct=$(grep -oE '[0-9]+\.[0-9]+% coverage' tarpaulin.log | head -1 | grep -oE '[0-9]+\.[0-9]+' || echo "")
          if [ -z "$coverage_pct" ]; then
            echo "::error::Coverage extraction FAILED for TUI - output missing or corrupt"
            exit 1
          fi
          echo "=== Coverage Summary ==="
          echo "TUI coverage: ${coverage_pct}%"
          python $GITHUB_WORKSPACE/scripts/check-coverage.py tui ${coverage_pct}

  # ============================================================
  # Lint & Format (polyglot)
  # ============================================================
  lint:
    name: Lint & Format
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for --new-from-rev

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy, rustfmt

      # Gateway (Node.js) - ESLint & Prettier
      - name: Install Gateway dependencies
        run: |
          cd src/services/gateway
          npm ci --ignore-scripts || npm install --ignore-scripts

      - name: Lint Gateway (ESLint)
        run: |
          cd src/services/gateway
          npm run lint || echo "::warning::ESLint found issues in gateway"

      - name: Format check Gateway (Prettier)
        run: |
          cd src/services/gateway
          npm run format:check || echo "::warning::Prettier found formatting issues in gateway"

      # Processor (Python) - Ruff & Black
      - name: Install Python linters
        run: pip install ruff black

      - name: Lint Processor (Ruff) - new code only
        run: |
          cd src/services/processor
          ruff check . --diff || echo "::warning::Ruff found issues in processor"

      - name: Format check Processor (Black)
        run: |
          cd src/services/processor
          black --check . || echo "::warning::Black found formatting issues in processor"

      # Go services - golangci-lint (new code only)
      - name: Install golangci-lint
        uses: golangci/golangci-lint-action@v4
        with:
          version: v1.55
          args: --new-from-rev=origin/main
          working-directory: src/services/metrics-engine
        continue-on-error: true

      - name: Lint read-model (golangci-lint)
        uses: golangci/golangci-lint-action@v4
        with:
          version: v1.55
          args: --new-from-rev=origin/main
          working-directory: src/services/read-model
        continue-on-error: true

      # Rust TUI - Clippy & rustfmt
      - name: Lint TUI (Clippy)
        run: |
          cd src/interfaces/tui
          cargo clippy -- -D warnings 2>&1 || echo "::warning::Clippy found issues in TUI"

      - name: Format check TUI (rustfmt)
        run: |
          cd src/interfaces/tui
          cargo fmt --check || echo "::warning::rustfmt found formatting issues in TUI"

      # Commitlint - verify last commit
      - name: Install root dependencies
        run: npm ci || npm install

      - name: Lint commit message
        run: npm run lint:commits || echo "::warning::Commit message does not follow Conventional Commits"

  # ============================================================
  # Release (Future extensibility)
  # ============================================================
  # release:
  #   name: Create Release
  #   runs-on: ubuntu-latest
  #   needs: [bazel, docker]
  #   if: startsWith(github.ref, 'refs/tags/v')
  #   steps:
  #     - uses: actions/checkout@v4
  #     - name: Create GitHub Release
  #       uses: softprops/action-gh-release@v1
